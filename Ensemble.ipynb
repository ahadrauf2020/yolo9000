{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import shutil \n",
    "# reference : https://github.com/automan000/CyclicLR_Scheduler_PyTorch\n",
    "from CyclicLR_Scheduler_PyTorch.cyclic_lr_scheduler import CyclicLR\n",
    "from Residual_Attention_Network.model.residual_attention_network import ResidualAttentionModel_92_32input_update as ResidualAttentionModel\n",
    "import resnet_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 100000, 'val': 10000, 'test': 10000}\n"
     ]
    }
   ],
   "source": [
    "# Load the Data\n",
    "data_dir = './modified_data/tiny-imagenet-200'\n",
    "num_classes = 200\n",
    "\n",
    "# Create the training data generator\n",
    "batch_size = 500\n",
    "im_height = 64\n",
    "im_width = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_data(batch_size=500):\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0, 0, 0), tuple(np.sqrt((255, 255, 255)))),\n",
    "    ])\n",
    "\n",
    "    # Load Data from folders\n",
    "    image_datasets = {\n",
    "        'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=data_transforms),\n",
    "        'val': datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=data_transforms),\n",
    "        'test': datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=data_transforms)\n",
    "    }\n",
    "\n",
    "    # subset_indices = np.random.permutation(range(100))\n",
    "    # dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=False, \n",
    "    #                              sampler=SubsetRandomSampler(subset_indices)) for x in phases}\n",
    "\n",
    "    dataloaders = {'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True),\n",
    "                  'val': DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True),\n",
    "                  'test': DataLoader(image_datasets['test'], batch_size=batch_size, shuffle=False)}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in phases}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    return image_datasets, dataloaders, dataset_sizes, class_names\n",
    "\n",
    "image_datasets, dataloaders, dataset_sizes, class_names = load_data()\n",
    "\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    tr_acc, val_acc = [], []\n",
    "    tr_loss, val_loss  = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "                if phase == 'train':\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            if phase == 'train':\n",
    "                tr_acc.append(epoch_acc)\n",
    "                tr_loss.append(epoch_loss)\n",
    "            elif phase == 'val':\n",
    "                val_acc.append(epoch_acc)\n",
    "                val_loss.append(epoch_loss)\n",
    "                \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, tr_acc, val_acc, tr_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=200, bias=True)\n",
       "    (1): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "# Freeze model parameters to train only the last layer. \n",
    "# Comment out this cell if you want to fine tune the whole network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 3.7194 Acc: 0.2345\n",
      "val Loss: 3.1500 Acc: 0.3170\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 2.9385 Acc: 0.3467\n",
      "val Loss: 3.0045 Acc: 0.3407\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 2.7716 Acc: 0.3736\n",
      "val Loss: 2.9721 Acc: 0.3381\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 2.6766 Acc: 0.3886\n",
      "val Loss: 2.9419 Acc: 0.3480\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 2.6167 Acc: 0.3986\n",
      "val Loss: 2.9417 Acc: 0.3502\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 2.4794 Acc: 0.4265\n",
      "val Loss: 2.9084 Acc: 0.3559\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 2.4716 Acc: 0.4283\n",
      "val Loss: 2.9089 Acc: 0.3565\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 2.4670 Acc: 0.4300\n",
      "val Loss: 2.9055 Acc: 0.3581\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 2.4624 Acc: 0.4305\n",
      "val Loss: 2.9155 Acc: 0.3543\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 2.4564 Acc: 0.4319\n",
      "val Loss: 2.9058 Acc: 0.3553\n",
      "\n",
      "Training complete in 46m 10s\n",
      "Best val Acc: 0.358100\n"
     ]
    }
   ],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# learning_rates = [0.00001, 0.0001, 0.001]\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)\n",
    "model, tr_acc, val_acc, tr_loss, val_loss = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './models/resnet18_model2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and saved state_dict\n",
    "vgg_model = models.vgg11_bn(pretrained=True)\n",
    "num_ftrs = vgg_model.classifier[6].in_features\n",
    "vgg_model.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "vgg_model.load_state_dict(torch.load('./models/vgg11_bn_best_model_state_dict.pt'))\n",
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet_model.load_state_dict(torch.load('./models/resnet18_model_state_dict.pt'))\n",
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer25): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer26): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer27): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer28): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer29): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer30): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer31): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer32): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer25): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer26): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer27): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer28): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer29): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer30): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer31): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer32): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1664, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model = torchvision.models.densenet169(pretrained=True)\n",
    "num_ftrs = dense_model.classifier.in_features\n",
    "dense_model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "dense_model.load_state_dict(torch.load('./models/dense169_model_state_dict.pt'))\n",
    "dense_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in dense_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Load models onto GPU\n",
    "resnet_model = resnet_model.to(device)\n",
    "vgg_model = vgg_model.to(device)\n",
    "dense_model = dense_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble():\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        self.loss = 0.0\n",
    "        self.acc = 0.0\n",
    "        \n",
    "    def find_majority_vote(self, preds):\n",
    "        maj_vote = torch.zeros(preds.shape[1])\n",
    "        for i in range(preds.shape[1]):\n",
    "            _, counts = np.unique(preds[:, i], return_counts=True)\n",
    "            maj_vote[i] = preds[np.argmax(counts), i]\n",
    "        maj_vote = maj_vote.to(device)\n",
    "        return maj_vote\n",
    "        \n",
    "    def evaluate_all(self, criterion, mode='average'):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        phase = 'val'\n",
    "        for m in self.models:\n",
    "            m.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                if mode == 'average':\n",
    "                    # Take average of the output to make prediction\n",
    "                    outputs = None\n",
    "                    for m in self.models:\n",
    "                        if outputs is None:\n",
    "                            outputs = m(inputs)\n",
    "                        else:\n",
    "                            outputs += m(inputs)\n",
    "                    outputs /= len(self.models)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    # Majority vote\n",
    "                    loss = 0\n",
    "                    predictions = torch.zeros(len(self.models), inputs.shape[0])\n",
    "                    for i in range(len(self.models)):\n",
    "                        outputs = self.models[i](inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        predictions[i, :] = preds\n",
    "                        loss += criterion(outputs, labels)\n",
    "                    preds = self.find_majority_vote(predictions)\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "#                 print(preds[:10])\n",
    "#                 print(labels.data[:10])\n",
    "#                 print()\n",
    "            self.loss = running_loss / dataset_sizes[phase]\n",
    "            self.acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        return self.acc, self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averaging\n",
      "validation accuracy tensor(0.5321, device='cuda:0', dtype=torch.float64)\n",
      "validation loss 1.9333795607089996\n",
      "\n",
      "averaging\n",
      "validation accuracy tensor(0.5360, device='cuda:0', dtype=torch.float64)\n",
      "validation loss 1.9644196271896361\n",
      "\n",
      "averaging\n",
      "validation accuracy tensor(0.5200, device='cuda:0', dtype=torch.float64)\n",
      "validation loss 2.013554871082306\n",
      "\n",
      "averaging\n",
      "validation accuracy tensor(0.5697, device='cuda:0', dtype=torch.float64)\n",
      "validation loss 1.7308927834033967\n",
      "\n",
      "majority vote\n",
      "validation accuracy tensor(0.4447, device='cuda:0', dtype=torch.float64)\n",
      "validation loss 7.863580560684204\n"
     ]
    }
   ],
   "source": [
    "# Ensemble by averaging\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ensemble_solver = Ensemble([resnet_model, vgg_model])\n",
    "val_acc, val_loss = ensemble_solver.evaluate_all(criterion)\n",
    "print(\"averaging\")\n",
    "print(\"validation accuracy\", val_acc)\n",
    "print(\"validation loss\", val_loss)\n",
    "print()\n",
    "\n",
    "# Ensemble by averaging\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ensemble_solver = Ensemble([resnet_model, dense_model])\n",
    "val_acc, val_loss = ensemble_solver.evaluate_all(criterion)\n",
    "print(\"averaging\")\n",
    "print(\"validation accuracy\", val_acc)\n",
    "print(\"validation loss\", val_loss)\n",
    "print()\n",
    "\n",
    "# Ensemble by averaging\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ensemble_solver = Ensemble([vgg_model, dense_model])\n",
    "val_acc, val_loss = ensemble_solver.evaluate_all(criterion)\n",
    "print(\"averaging\")\n",
    "print(\"validation accuracy\", val_acc)\n",
    "print(\"validation loss\", val_loss)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Ensemble by averaging\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ensemble_solver = Ensemble([resnet_model, dense_model, vgg_model])\n",
    "val_acc, val_loss = ensemble_solver.evaluate_all(criterion)\n",
    "print(\"averaging\")\n",
    "print(\"validation accuracy\", val_acc)\n",
    "print(\"validation loss\", val_loss)\n",
    "print()\n",
    "\n",
    "# Ensemble by majority vote\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ensemble_solver = Ensemble([resnet_model, dense_model, vgg_model])\n",
    "val_acc, val_loss = ensemble_solver.evaluate_all(criterion, 'majority vote')\n",
    "print(\"majority vote\")\n",
    "print(\"validation accuracy\", val_acc)\n",
    "print(\"validation loss\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_ensemble_model(snapshots):\n",
    "\n",
    "def snapshot_training(model, criterion, optimizer, scheduler, num_epochs=25, step_size=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    tr_acc, val_acc = [], []\n",
    "    tr_loss, val_loss  = [], []\n",
    "    num_snapshots = num_epochs // (step_size * 2)\n",
    "    print(\"num_snapshots\", num_snapshots)\n",
    "    snapshots = []\n",
    "    current_lr = scheduler.get_lr()[0]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        print(\"current lr\", current_lr)\n",
    "        \n",
    "        # Check if we take snapshot        \n",
    "        take_snapshot = current_lr == scheduler.base_lr\n",
    "        if take_snapshot:\n",
    "            print(\"at the bottom!\")\n",
    "\n",
    "            \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            next_ind = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                if phase == 'train':\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            # Update scheduler\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                current_lr = scheduler.get_lr()[0]\n",
    "                \n",
    "            # Update loss and acuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            if phase == 'train':\n",
    "                tr_acc.append(epoch_acc)\n",
    "                tr_loss.append(epoch_loss)\n",
    "            elif phase == 'val':\n",
    "                val_acc.append(epoch_acc)\n",
    "                val_loss.append(epoch_loss)\n",
    "                \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "        print()\n",
    "        if take_snapshot:\n",
    "            print(\"snapshot taken\")\n",
    "            snapshots.append(copy.deepcopy(model.state_dict()))\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    return snapshots, tr_acc, val_acc, tr_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "resnet_model = models.resnet18(pretrained=True)\n",
    "# for param in resnet_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# resnet_model.load_state_dict(torch.load('./models/resnet18_model_state_dict.pt'))\n",
    "resnet_model = resnet_model.to(device)\n",
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_snapshots 2\n",
      "Epoch 0/21\n",
      "----------\n",
      "current lr 0.021\n",
      "train Loss: 4.8489 Acc: 0.0791\n",
      "val Loss: 4.0772 Acc: 0.1873\n",
      "\n",
      "Epoch 1/21\n",
      "----------\n",
      "current lr 0.001\n",
      "at the bottom!\n",
      "train Loss: 3.5228 Acc: 0.2788\n",
      "val Loss: 3.1350 Acc: 0.3258\n",
      "\n",
      "snapshot taken\n",
      "Epoch 2/21\n",
      "----------\n",
      "current lr 0.020602\n",
      "train Loss: 2.2223 Acc: 0.4599\n",
      "val Loss: 2.1512 Acc: 0.4756\n",
      "\n",
      "Epoch 3/21\n",
      "----------\n",
      "current lr 0.03981195999999999\n",
      "train Loss: 1.8626 Acc: 0.5294\n",
      "val Loss: 2.4376 Acc: 0.4241\n",
      "\n",
      "Epoch 4/21\n",
      "----------\n",
      "current lr 0.05863576060000001\n",
      "train Loss: 1.7787 Acc: 0.5489\n",
      "val Loss: 2.7045 Acc: 0.3882\n",
      "\n",
      "Epoch 5/21\n",
      "----------\n",
      "current lr 0.07707920399200001\n",
      "train Loss: 1.6978 Acc: 0.5671\n",
      "val Loss: 3.1626 Acc: 0.3473\n",
      "\n",
      "Epoch 6/21\n",
      "----------\n",
      "current lr 0.0951480149401\n",
      "train Loss: 1.5795 Acc: 0.5944\n",
      "val Loss: 2.9106 Acc: 0.3793\n",
      "\n",
      "Epoch 7/21\n",
      "----------\n",
      "current lr 0.0755652278325592\n",
      "train Loss: 0.9086 Acc: 0.7472\n",
      "val Loss: 3.3432 Acc: 0.3691\n",
      "\n",
      "Epoch 8/21\n",
      "----------\n",
      "current lr 0.05636468166567521\n",
      "train Loss: 0.3558 Acc: 0.8958\n",
      "val Loss: 2.7100 Acc: 0.4788\n",
      "\n",
      "Epoch 9/21\n",
      "----------\n",
      "current lr 0.037540689899345635\n",
      "train Loss: 0.0736 Acc: 0.9844\n",
      "val Loss: 2.5216 Acc: 0.5174\n",
      "\n",
      "Epoch 10/21\n",
      "----------\n",
      "current lr 0.019087641500176087\n",
      "train Loss: 0.0149 Acc: 0.9992\n",
      "val Loss: 2.5157 Acc: 0.5236\n",
      "\n",
      "Epoch 11/21\n",
      "----------\n",
      "current lr 0.001\n",
      "at the bottom!\n",
      "train Loss: 0.0090 Acc: 0.9997\n",
      "val Loss: 2.5160 Acc: 0.5239\n",
      "\n",
      "snapshot taken\n",
      "Epoch 12/21\n",
      "----------\n",
      "current lr 0.018727697434322604\n",
      "train Loss: 0.0087 Acc: 0.9997\n",
      "val Loss: 2.5362 Acc: 0.5244\n",
      "\n",
      "Epoch 13/21\n",
      "----------\n",
      "current lr 0.036100840919958715\n",
      "train Loss: 0.0074 Acc: 0.9997\n",
      "val Loss: 2.5760 Acc: 0.5253\n",
      "\n",
      "Epoch 14/21\n",
      "----------\n",
      "current lr 0.0531247487661387\n",
      "train Loss: 0.0059 Acc: 0.9997\n",
      "val Loss: 2.6125 Acc: 0.5227\n",
      "\n",
      "Epoch 15/21\n",
      "----------\n",
      "current lr 0.06980466837130307\n",
      "train Loss: 0.0047 Acc: 0.9997\n",
      "val Loss: 2.6538 Acc: 0.5226\n",
      "\n",
      "Epoch 16/21\n",
      "----------\n",
      "current lr 0.08614577710948756\n",
      "train Loss: 0.0038 Acc: 0.9997\n",
      "val Loss: 2.6936 Acc: 0.5235\n",
      "\n",
      "Epoch 17/21\n",
      "----------\n",
      "current lr 0.06843545547071414\n",
      "train Loss: 0.0029 Acc: 0.9998\n",
      "val Loss: 2.7154 Acc: 0.5227\n",
      "\n",
      "Epoch 18/21\n",
      "----------\n",
      "current lr 0.05107082568700526\n",
      "train Loss: 0.0023 Acc: 0.9997\n",
      "val Loss: 2.7281 Acc: 0.5252\n",
      "\n",
      "Epoch 19/21\n",
      "----------\n",
      "current lr 0.03404674495342346\n",
      "train Loss: 0.0019 Acc: 0.9997\n",
      "val Loss: 2.7382 Acc: 0.5238\n",
      "\n",
      "Epoch 20/21\n",
      "----------\n",
      "current lr 0.017358138751944634\n",
      "train Loss: 0.0017 Acc: 0.9997\n",
      "val Loss: 2.7370 Acc: 0.5246\n",
      "\n",
      "Epoch 21/21\n",
      "----------\n",
      "current lr 0.001\n",
      "at the bottom!\n",
      "train Loss: 0.0015 Acc: 0.9998\n",
      "val Loss: 2.7371 Acc: 0.5262\n",
      "\n",
      "snapshot taken\n",
      "Training complete in 116m 42s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(resnet_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# cyc_lr_scheduler = lr_scheduler.CyclicLR(optimizer_ft, base_lr=1e-4, max_lr=1e-1, step_size_up=10)\n",
    "cyc_lr_scheduler = CyclicLR(optimizer_ft, base_lr=0.001, max_lr=0.1, step_size=5, mode='exp_range')\n",
    "# cyc_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)\n",
    "snapshots, tr_acc, val_acc, tr_loss, val_loss = snapshot_training(resnet_model, criterion, \n",
    "                                                                     optimizer_ft, cyc_lr_scheduler, num_epochs=22, step_size=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_models = []\n",
    "for i in range(len(snapshots)):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "#     model.load_state_dict(torch.load('./models/snapshots/resnet18_snapshot{}.pt'.format(i)))\n",
    "    model.load_state_dict()\n",
    "    model = model.to(device)\n",
    "    sn_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble by averaging\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ensemble_solver = Ensemble(sn_models)\n",
    "val_acc, val_loss = ensemble_solver.evaluate_all(criterion)\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for snapshot ensemble\n",
    "\n",
    "(tune only the last layer)  \n",
    "base_lr: 0.0001  \n",
    "max_lr:0.1  \n",
    "num of epochs: 22  \n",
    "stepsize: 5  \n",
    "num of snapshots: 2  \n",
    "validation accuracy without snapshot ensemble: 35.2%    \n",
    "validation accuracy with snapshot ensemble: 32.57%  \n",
    "\n",
    "\n",
    "(fine tune the whole layers)  \n",
    "base_lr: 0.001  \n",
    "max_lr:0.1  \n",
    "num of epochs: 22  \n",
    "stepsize: 5  \n",
    "num of snapshots: 2  \n",
    "validation accuracy without snapshot ensemble: 53.3%   \n",
    "validation accuracy :  53.1%\n",
    "\n",
    "(fine tune the whole layers)   \n",
    "base_lr: 0.001  \n",
    "max_lr:0.1  \n",
    "num of epochs: 28  \n",
    "stepsize: 3  \n",
    "num of snapshots: 4  \n",
    "validation accuracy without snapshot ensemble: 53.3%   \n",
    "validation accuracy :  53.51%  \n",
    "\n",
    "(fine tune the whole layers)  \n",
    "base_lr: 0.0001   \n",
    "max_lr:0.1  \n",
    "stepsize 10  \n",
    "num of snapshots 3  \n",
    "validation accuracy without snapshot ensemble: 53.3%   \n",
    "validation accuracy : 52.59%    \n",
    "\n",
    "(fine tune the whole layers)    \n",
    "base_lr: 0.001  \n",
    "max_lr:0.1  \n",
    "stepsize 15  \n",
    "num of snapshots 2  \n",
    "validation accuracy without snapshot ensemble: 53.3%   \n",
    "validation accuracy : 52.63 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_snapshots(model_list, snapshots, criterion):\n",
    "    phase = 'val'\n",
    "\n",
    "    for model, weight in zip(model_list, snapshots):\n",
    "        model.load_state_dict(weight)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "       \n",
    "    test_acc = 0.0\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataloaders[phase]:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output_list = [model(data).unsqueeze(0) for model in model_list]\n",
    "        output = torch.mean(torch.cat(output_list), 0).squeeze()\n",
    "        test_loss += criterion(output, target)\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += torch.sum(pred == target.data)\n",
    "        \n",
    "    test_acc = correct.double() / dataset_sizes[phase]\n",
    "    test_loss /= dataset_sizes[phase]\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, test_loss, test_acc))\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0060 Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0060, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.5263, device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = []\n",
    "for i in range(len(snapshots)):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    model_list.append(model)\n",
    "evaluate_snapshots(model_list, snapshots, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_snapshots 2\n",
      "Epoch 0/43\n",
      "----------\n",
      "current lr 0.010190909090909089\n",
      "train Loss: 0.0015 Acc: 0.9998\n",
      "val Loss: 2.7897 Acc: 0.5253\n",
      "\n",
      "Epoch 1/43\n",
      "----------\n",
      "current lr 0.0001\n",
      "at the bottom!\n",
      "train Loss: 0.0016 Acc: 0.9998\n",
      "val Loss: 2.7920 Acc: 0.5239\n",
      "\n",
      "snapshot taken\n",
      "Epoch 2/43\n",
      "----------\n",
      "current lr 0.009990100000000009\n",
      "train Loss: 0.0015 Acc: 0.9998\n",
      "val Loss: 2.7890 Acc: 0.5239\n",
      "\n",
      "Epoch 3/43\n",
      "----------\n",
      "current lr 0.019682397999999997\n",
      "train Loss: 0.0014 Acc: 0.9998\n",
      "val Loss: 2.7870 Acc: 0.5243\n",
      "\n",
      "Epoch 4/43\n",
      "----------\n",
      "current lr 0.029179861030000004\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 2.7911 Acc: 0.5252\n",
      "\n",
      "Epoch 5/43\n",
      "----------\n",
      "current lr 0.0384854165596\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 2.7998 Acc: 0.5256\n",
      "\n",
      "Epoch 6/43\n",
      "----------\n",
      "current lr 0.047601952992505\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 2.8097 Acc: 0.5247\n",
      "\n",
      "Epoch 7/43\n",
      "----------\n",
      "current lr 0.05653232015509595\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 2.8184 Acc: 0.5247\n",
      "\n",
      "Epoch 8/43\n",
      "----------\n",
      "current lr 0.0652793297791358\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 2.8312 Acc: 0.5232\n",
      "\n",
      "Epoch 9/43\n",
      "----------\n",
      "current lr 0.07384575597867937\n",
      "train Loss: 0.0014 Acc: 0.9997\n",
      "val Loss: 2.8406 Acc: 0.5256\n",
      "\n",
      "Epoch 10/43\n",
      "----------\n",
      "current lr 0.08223433572125415\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 2.8498 Acc: 0.5257\n",
      "\n",
      "Epoch 11/43\n",
      "----------\n",
      "current lr 0.09044776929337957\n",
      "train Loss: 0.0013 Acc: 0.9997\n",
      "val Loss: 2.8700 Acc: 0.5241\n",
      "\n",
      "Epoch 12/43\n",
      "----------\n",
      "current lr 0.08059986244040118\n",
      "train Loss: 0.0013 Acc: 0.9996\n",
      "val Loss: 2.8799 Acc: 0.5229\n",
      "\n",
      "Epoch 13/43\n",
      "----------\n",
      "current lr 0.07093987894755306\n",
      "train Loss: 0.0011 Acc: 0.9997\n",
      "val Loss: 2.8792 Acc: 0.5257\n",
      "\n",
      "Epoch 14/43\n",
      "----------\n",
      "current lr 0.06146504513831782\n",
      "train Loss: 0.0010 Acc: 0.9997\n",
      "val Loss: 2.8863 Acc: 0.5263\n",
      "\n",
      "Epoch 15/43\n",
      "----------\n",
      "current lr 0.05217262401737256\n",
      "train Loss: 0.0009 Acc: 0.9997\n",
      "val Loss: 2.8870 Acc: 0.5257\n",
      "\n",
      "Epoch 16/43\n",
      "----------\n",
      "current lr 0.04305991481433236\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.8931 Acc: 0.5253\n",
      "\n",
      "Epoch 17/43\n",
      "----------\n",
      "current lr 0.034124252532951226\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.8976 Acc: 0.5239\n",
      "\n",
      "Epoch 18/43\n",
      "----------\n",
      "current lr 0.025363007505716288\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9008 Acc: 0.5251\n",
      "\n",
      "Epoch 19/43\n",
      "----------\n",
      "current lr 0.016773584953772746\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9019 Acc: 0.5246\n",
      "\n",
      "Epoch 20/43\n",
      "----------\n",
      "current lr 0.008353424552117518\n",
      "train Loss: 0.0007 Acc: 0.9998\n",
      "val Loss: 2.9017 Acc: 0.5257\n",
      "\n",
      "Epoch 21/43\n",
      "----------\n",
      "current lr 0.0001\n",
      "at the bottom!\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9028 Acc: 0.5253\n",
      "\n",
      "snapshot taken\n",
      "Epoch 22/43\n",
      "----------\n",
      "current lr 0.008189181403530378\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9020 Acc: 0.5252\n",
      "\n",
      "Epoch 23/43\n",
      "----------\n",
      "current lr 0.01611657917899015\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9016 Acc: 0.5254\n",
      "\n",
      "Epoch 24/43\n",
      "----------\n",
      "current lr 0.023884620080800337\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9031 Acc: 0.5247\n",
      "\n",
      "Epoch 25/43\n",
      "----------\n",
      "current lr 0.03149569850665646\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9135 Acc: 0.5247\n",
      "\n",
      "Epoch 26/43\n",
      "----------\n",
      "current lr 0.03895217690198738\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9130 Acc: 0.5250\n",
      "\n",
      "Epoch 27/43\n",
      "----------\n",
      "current lr 0.04625638615956101\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9114 Acc: 0.5252\n",
      "\n",
      "Epoch 28/43\n",
      "----------\n",
      "current lr 0.05341062601429297\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9186 Acc: 0.5255\n",
      "\n",
      "Epoch 29/43\n",
      "----------\n",
      "current lr 0.06041716543331431\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9223 Acc: 0.5250\n",
      "\n",
      "Epoch 30/43\n",
      "----------\n",
      "current lr 0.06727824300135381\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9283 Acc: 0.5266\n",
      "\n",
      "Epoch 31/43\n",
      "----------\n",
      "current lr 0.0739960673014892\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9373 Acc: 0.5231\n",
      "\n",
      "Epoch 32/43\n",
      "----------\n",
      "current lr 0.06594139596562687\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9438 Acc: 0.5258\n",
      "\n",
      "Epoch 33/43\n",
      "----------\n",
      "current lr 0.05804042844975164\n",
      "train Loss: 0.0008 Acc: 0.9997\n",
      "val Loss: 2.9445 Acc: 0.5246\n",
      "\n",
      "Epoch 34/43\n",
      "----------\n",
      "current lr 0.050290896144597384\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9529 Acc: 0.5253\n",
      "\n",
      "Epoch 35/43\n",
      "----------\n",
      "current lr 0.0426905604427012\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9559 Acc: 0.5258\n",
      "\n",
      "Epoch 36/43\n",
      "----------\n",
      "current lr 0.03523721236522848\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9593 Acc: 0.5263\n",
      "\n",
      "Epoch 37/43\n",
      "----------\n",
      "current lr 0.02792867219326095\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9570 Acc: 0.5254\n",
      "\n",
      "Epoch 38/43\n",
      "----------\n",
      "current lr 0.020762789103496247\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9636 Acc: 0.5256\n",
      "\n",
      "Epoch 39/43\n",
      "----------\n",
      "current lr 0.013737440808307544\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9606 Acc: 0.5270\n",
      "\n",
      "Epoch 40/43\n",
      "----------\n",
      "current lr 0.006850533200112235\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9550 Acc: 0.5255\n",
      "\n",
      "Epoch 41/43\n",
      "----------\n",
      "current lr 0.0001\n",
      "at the bottom!\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9579 Acc: 0.5264\n",
      "\n",
      "snapshot taken\n",
      "Epoch 42/43\n",
      "----------\n",
      "current lr 0.006716197589429971\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 2.9602 Acc: 0.5258\n",
      "\n",
      "Epoch 43/43\n",
      "----------\n",
      "current lr 0.0132000712270714\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9588 Acc: 0.5255\n",
      "\n",
      "Training complete in 233m 31s\n",
      "Best val Acc: 0.000000\n",
      "*******************\n",
      "stepsize 10\n",
      "num of snapshots 3\n",
      "val Loss: 0.0058 Acc: 0.5259\n",
      "num_snapshots 1\n",
      "Epoch 0/43\n",
      "----------\n",
      "current lr 0.006827272727272726\n",
      "train Loss: 0.0005 Acc: 0.9998\n",
      "val Loss: 2.9588 Acc: 0.5257\n",
      "\n",
      "Epoch 1/43\n",
      "----------\n",
      "current lr 0.0001\n",
      "at the bottom!\n",
      "train Loss: 0.0006 Acc: 0.9998\n",
      "val Loss: 2.9556 Acc: 0.5254\n",
      "\n",
      "snapshot taken\n",
      "Epoch 2/43\n",
      "----------\n",
      "current lr 0.006693399999999999\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9599 Acc: 0.5265\n",
      "\n",
      "Epoch 3/43\n",
      "----------\n",
      "current lr 0.013154931999999996\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9668 Acc: 0.5251\n",
      "\n",
      "Epoch 4/43\n",
      "----------\n",
      "current lr 0.019486574019999998\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9636 Acc: 0.5260\n",
      "\n",
      "Epoch 5/43\n",
      "----------\n",
      "current lr 0.02569027770639999\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9622 Acc: 0.5265\n",
      "\n",
      "Epoch 6/43\n",
      "----------\n",
      "current lr 0.03176796866167\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9683 Acc: 0.5251\n",
      "\n",
      "Epoch 7/43\n",
      "----------\n",
      "current lr 0.03772154677006396\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9702 Acc: 0.5267\n",
      "\n",
      "Epoch 8/43\n",
      "----------\n",
      "current lr 0.043552886519423885\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9773 Acc: 0.5253\n",
      "\n",
      "Epoch 9/43\n",
      "----------\n",
      "current lr 0.04926383731911957\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 2.9761 Acc: 0.5264\n",
      "\n",
      "Epoch 10/43\n",
      "----------\n",
      "current lr 0.05485622381416944\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9793 Acc: 0.5256\n",
      "\n",
      "Epoch 11/43\n",
      "----------\n",
      "current lr 0.06033184619558637\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9817 Acc: 0.5264\n",
      "\n",
      "Epoch 12/43\n",
      "----------\n",
      "current lr 0.06569248050699357\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9832 Acc: 0.5268\n",
      "\n",
      "Epoch 13/43\n",
      "----------\n",
      "current lr 0.07093987894755306\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9943 Acc: 0.5261\n",
      "\n",
      "Epoch 14/43\n",
      "----------\n",
      "current lr 0.07607577017125064\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9997 Acc: 0.5261\n",
      "\n",
      "Epoch 15/43\n",
      "----------\n",
      "current lr 0.08110185958257954\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 3.0011 Acc: 0.5251\n",
      "\n",
      "Epoch 16/43\n",
      "----------\n",
      "current lr 0.08601982962866472\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 2.9939 Acc: 0.5254\n",
      "\n",
      "Epoch 17/43\n",
      "----------\n",
      "current lr 0.0794899225768862\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 3.0095 Acc: 0.5267\n",
      "\n",
      "Epoch 18/43\n",
      "----------\n",
      "current lr 0.07308202168318038\n",
      "train Loss: 0.0007 Acc: 0.9997\n",
      "val Loss: 3.0038 Acc: 0.5244\n",
      "\n",
      "Epoch 19/43\n",
      "----------\n",
      "current lr 0.066794339815091\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 3.0135 Acc: 0.5268\n",
      "\n",
      "Epoch 20/43\n",
      "----------\n",
      "current lr 0.06062511338219508\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 3.0124 Acc: 0.5260\n",
      "\n",
      "Epoch 21/43\n",
      "----------\n",
      "current lr 0.05457260204397558\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 3.0164 Acc: 0.5274\n",
      "\n",
      "Epoch 22/43\n",
      "----------\n",
      "current lr 0.048635088421182235\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 3.0186 Acc: 0.5251\n",
      "\n",
      "Epoch 23/43\n",
      "----------\n",
      "current lr 0.04281087781064038\n",
      "train Loss: 0.0006 Acc: 0.9997\n",
      "val Loss: 3.0243 Acc: 0.5254\n",
      "\n",
      "Epoch 24/43\n",
      "----------\n",
      "current lr 0.037098297903467206\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0267 Acc: 0.5257\n",
      "\n",
      "Epoch 25/43\n",
      "----------\n",
      "current lr 0.03149569850665646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0246 Acc: 0.5255\n",
      "\n",
      "Epoch 26/43\n",
      "----------\n",
      "current lr 0.02600145126799158\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0255 Acc: 0.5259\n",
      "\n",
      "Epoch 27/43\n",
      "----------\n",
      "current lr 0.020613949404249324\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0211 Acc: 0.5268\n",
      "\n",
      "Epoch 28/43\n",
      "----------\n",
      "current lr 0.015331607432655126\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0255 Acc: 0.5260\n",
      "\n",
      "Epoch 29/43\n",
      "----------\n",
      "current lr 0.010152860905552383\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0246 Acc: 0.5257\n",
      "\n",
      "Epoch 30/43\n",
      "----------\n",
      "current lr 0.005076166148248429\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0343 Acc: 0.5259\n",
      "\n",
      "Epoch 31/43\n",
      "----------\n",
      "current lr 0.0001\n",
      "at the bottom!\n",
      "train Loss: 0.0005 Acc: 0.9998\n",
      "val Loss: 3.0293 Acc: 0.5265\n",
      "\n",
      "snapshot taken\n",
      "Epoch 32/43\n",
      "----------\n",
      "current lr 0.004977140441898302\n",
      "train Loss: 0.0005 Acc: 0.9998\n",
      "val Loss: 3.0321 Acc: 0.5262\n",
      "\n",
      "Epoch 33/43\n",
      "----------\n",
      "current lr 0.009756738074958604\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0334 Acc: 0.5249\n",
      "\n",
      "Epoch 34/43\n",
      "----------\n",
      "current lr 0.014440256041313545\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0278 Acc: 0.5264\n",
      "\n",
      "Epoch 35/43\n",
      "----------\n",
      "current lr 0.019029137974533854\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0322 Acc: 0.5261\n",
      "\n",
      "Epoch 36/43\n",
      "----------\n",
      "current lr 0.023524808243485663\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0311 Acc: 0.5264\n",
      "\n",
      "Epoch 37/43\n",
      "----------\n",
      "current lr 0.02792867219326095\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0300 Acc: 0.5260\n",
      "\n",
      "Epoch 38/43\n",
      "----------\n",
      "current lr 0.03224211638321642\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0370 Acc: 0.5267\n",
      "\n",
      "Epoch 39/43\n",
      "----------\n",
      "current lr 0.03646650882215341\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0348 Acc: 0.5262\n",
      "\n",
      "Epoch 40/43\n",
      "----------\n",
      "current lr 0.04060319920067337\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0356 Acc: 0.5262\n",
      "\n",
      "Epoch 41/43\n",
      "----------\n",
      "current lr 0.044653519120740705\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0384 Acc: 0.5268\n",
      "\n",
      "Epoch 42/43\n",
      "----------\n",
      "current lr 0.048618782322486634\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0375 Acc: 0.5255\n",
      "\n",
      "Epoch 43/43\n",
      "----------\n",
      "current lr 0.05250028490828555\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0458 Acc: 0.5278\n",
      "\n",
      "Training complete in 234m 46s\n",
      "Best val Acc: 0.000000\n",
      "*******************\n",
      "stepsize 15\n",
      "num of snapshots 2\n",
      "val Loss: 0.0060 Acc: 0.5263\n",
      "num_snapshots 1\n",
      "Epoch 0/43\n",
      "----------\n",
      "current lr 0.005145454545454551\n",
      "train Loss: 0.0004 Acc: 0.9998\n",
      "val Loss: 3.0411 Acc: 0.5270\n",
      "\n",
      "Epoch 1/43\n",
      "----------\n",
      "current lr 0.0001\n",
      "at the bottom!\n",
      "train Loss: 0.0004 Acc: 0.9998\n",
      "val Loss: 3.0424 Acc: 0.5264\n",
      "\n",
      "snapshot taken\n",
      "Epoch 2/43\n",
      "----------\n",
      "current lr 0.005045050000000005\n",
      "train Loss: 0.0005 Acc: 0.9998\n",
      "val Loss: 3.0394 Acc: 0.5266\n",
      "\n",
      "Epoch 3/43\n",
      "----------\n",
      "current lr 0.009891199000000008\n",
      "train Loss: 0.0005 Acc: 0.9998\n",
      "val Loss: 3.0480 Acc: 0.5249\n",
      "\n",
      "Epoch 4/43\n",
      "----------\n",
      "current lr 0.014639930514999991\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0458 Acc: 0.5261\n",
      "\n",
      "Epoch 5/43\n",
      "----------\n",
      "current lr 0.019292708279799998\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0445 Acc: 0.5255\n",
      "\n",
      "Epoch 6/43\n",
      "----------\n",
      "current lr 0.023850976496252497\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0435 Acc: 0.5258\n",
      "\n",
      "Epoch 7/43\n",
      "----------\n",
      "current lr 0.02831616007754797\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0453 Acc: 0.5262\n",
      "\n",
      "Epoch 8/43\n",
      "----------\n",
      "current lr 0.03268966488956791\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0511 Acc: 0.5260\n",
      "\n",
      "Epoch 9/43\n",
      "----------\n",
      "current lr 0.03697287798933969\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0526 Acc: 0.5268\n",
      "\n",
      "Epoch 10/43\n",
      "----------\n",
      "current lr 0.041167167860627074\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0494 Acc: 0.5255\n",
      "\n",
      "Epoch 11/43\n",
      "----------\n",
      "current lr 0.045273884646689785\n",
      "train Loss: 0.0005 Acc: 0.9997\n",
      "val Loss: 3.0528 Acc: 0.5261\n",
      "\n",
      "Epoch 12/43\n",
      "----------\n",
      "current lr 0.04929436038024518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-7006cbee6554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcyc_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCyclicLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exp_range'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     snapshots, tr_acc, val_acc, tr_loss, val_loss = snapshot_training(resnet_model, criterion, \n\u001b[0;32m----> 8\u001b[0;31m                                                                          optimizer_ft, cyc_lr_scheduler, num_epochs=44, step_size=st)\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*******************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stepsize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-8a73faf77932>\u001b[0m in \u001b[0;36msnapshot_training\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, step_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stepsizes = [10, 15, 20]\n",
    "for st in stepsizes:\n",
    "    # Define Optimizer and Loss Function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.SGD(resnet_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    cyc_lr_scheduler = CyclicLR(optimizer_ft, base_lr=0.0001, max_lr=0.1, step_size=st, mode='exp_range')\n",
    "    snapshots, tr_acc, val_acc, tr_loss, val_loss = snapshot_training(resnet_model, criterion, \n",
    "                                                                         optimizer_ft, cyc_lr_scheduler, num_epochs=44, step_size=st)\n",
    "    print(\"*******************\")\n",
    "    print(\"stepsize\", st)\n",
    "    print(\"num of snapshots\", len(snapshots))\n",
    "    model_list = []\n",
    "    for i in range(len(snapshots)):\n",
    "        model = models.resnet18(pretrained=True)\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        model_list.append(model)\n",
    "    evaluate_snapshots(model_list, snapshots, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestEnsemble():\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        self.loss = 0.0\n",
    "        self.acc = 0.0\n",
    "        \n",
    "    def evaluate_all(self, criterion, mode='average'):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        phase = 'val'\n",
    "        for m in self.models:\n",
    "            m.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Take average of the output to make prediction\n",
    "                outputs = None\n",
    "                for m in self.models:\n",
    "                    if outputs is None:\n",
    "                        outputs = m(inputs)\n",
    "                    else:\n",
    "                        outputs += m(inputs)\n",
    "                outputs /= len(self.models)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            self.loss = running_loss / dataset_sizes[phase]\n",
    "            self.acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        return self.acc, self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fd43b1c1fb1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbest_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./models/resnet152_best_model_state_dict.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs182-proj/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs182-proj/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \"functionality.\".format(type(f)))\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "# Load Ahad's resnet152\n",
    "num_classes = 200\n",
    "model = resnet_modified.resnet152(pretrained=False, decay_factor=0.04278)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model = model.to(device)\n",
    "best_model_path = \"./models/resnet152_best_model_state_dict.pth\"\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=torch.device(device)))\n",
    "\n",
    "\n",
    "# Load Michael's dense162\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model_path = \"./models/dense_best.pth.tar\"\n",
    "state_dict = torch.load(best_model_path, map_location=torch.device(device))['state_dict']\n",
    "model.load_state_dict({k[7:]: v for k, v in state_dict.items()})\n",
    "model.eval()\n",
    "\n",
    "# checkpoint = torch.load('cs182_residual_attention_nn/model_best.pth.tar')\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# Load Chris's resnet attention\n",
    "model = ResidualAttentionModel()\n",
    "model =  torch.nn.DataParallel(model)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.001,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=1e-4)\n",
    "checkpoint = torch.load('./models/chris_resnet_model_best.pth.tar', map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load the classes\n",
    "    data_dir = pathlib.Path('./data/tiny-imagenet-200/train/')\n",
    "    CLASSES = sorted([item.name for item in data_dir.glob('*')])\n",
    "    im_height, im_width = 64, 64\n",
    "\n",
    "    ckpt = torch.load('latest.pt')\n",
    "    # model = Net(len(CLASSES), im_height, im_width)\n",
    "    model = resnet152(pretrained=False)\n",
    "    model.load_state_dict(ckpt['net'])\n",
    "    model.eval()\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#         transforms.Normalize((0, 0, 0), tuple(np.sqrt((255, 255, 255)))),\n",
    "    ])\n",
    "\n",
    "    # Loop through the CSV file and make a prediction for each line\n",
    "    with open('eval_classified.csv', 'w') as eval_output_file:  # Open the evaluation CSV file for writing\n",
    "        eval_dir = pathlib.Path('eval.csv')\n",
    "        for line in eval_dir.open():\n",
    "        # for line in pathlib.Path(sys.argv[1]).open():  # Open the input CSV file for reading\n",
    "            image_id, image_path, image_height, image_width, image_channels = line.strip().split(\n",
    "                ',')  # Extract CSV info\n",
    "\n",
    "            print(image_id, image_path, image_height, image_width, image_channels)\n",
    "            with open(image_path, 'rb') as f:\n",
    "                img = Image.open(f).convert('RGB')\n",
    "            img = data_transforms(img)[None, :]\n",
    "            outputs = model(img)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Write the prediction to the output file\n",
    "            eval_output_file.write('{},{}\\n'.format(image_id, CLASSES[predicted]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182-proj",
   "language": "python",
   "name": "cs182-proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
